import os
import sys
import json
import random
import logging
import traceback
import datetime as dt
from apify_client import ApifyClient
from dotenv import load_dotenv

# --- Setup ---
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)

dotenv_path = os.path.join(project_root, '.env')
load_dotenv(dotenv_path=dotenv_path)

from neurons.target_provider import get_scraping_targets
from scripts.etl_pipeline import main as run_etl
from common.data import DataSource
from storage.miner.sqlite_miner_storage import SqliteMinerStorage
from scripts.etl_pipeline import DATABASE_NAME

logging.basicConfig(level=logging.INFO, format='ðŸ¤– ORCHESTRATOR: %(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')

# --- Configuration ---
APIFY_TOKEN = os.getenv("APIFY_TOKEN")
STATE_FILE_PATH = os.path.join(project_root, "scripts", "pipeline_state.json")

X_SCRAPER_ACTOR_ID = "xtdata/twitter-x-scraper"
REDDIT_SCRAPER_ACTOR_ID = "trudax/reddit-scraper-lite"
YOUTUBE_DISCOVERY_ACTOR_ID = "streamers/youtube-scraper"
YOUTUBE_TRANSCRIPT_ACTOR_ID = "smartly_automated/youtube-transcript-scraper-premium-version"

# --- Cost Management Configuration ---
MONTHLY_BUDGET_USD = 135.0
RUNS_PER_DAY = 24 

COST_PER_1000_X = 0.50
COST_PER_1000_REDDIT = 2.00
COST_PER_1000_YOUTUBE_DISCOVERY = 5.00
COST_PER_1000_YOUTUBE_TRANSCRIPT = 20.00

def calculate_per_run_limits() -> dict:
    """Calculates the maximum number of items to scrape per run to stay within budget."""
    daily_budget = MONTHLY_BUDGET_USD / 30.0
    per_run_budget = daily_budget / RUNS_PER_DAY
    
    budget_x = per_run_budget * 0.4
    budget_reddit = per_run_budget * 0.4
    budget_youtube = per_run_budget * 0.2
    
    limit_x = int((budget_x / COST_PER_1000_X) * 1000)
    limit_reddit = int((budget_reddit / COST_PER_1000_REDDIT) * 1000)
    limit_youtube_videos = int((budget_youtube / (COST_PER_1000_YOUTUBE_DISCOVERY + COST_PER_1000_YOUTUBE_TRANSCRIPT)) * 1000)

    limits = {
        "x_per_run_limit": limit_x,
        "reddit_per_run_limit": limit_reddit,
        "youtube_per_run_limit": limit_youtube_videos,
    }
    logging.info(f"Calculated per-run scraping limits (for {RUNS_PER_DAY} runs/day): {limits}")
    return limits

# --- State Management (Watermarking) ---

def load_pipeline_state() -> dict:
    """Loads the last run timestamps from the state file."""
    if not os.path.exists(STATE_FILE_PATH): return {}
    try:
        with open(STATE_FILE_PATH, 'r') as f:
            state = json.load(f)
            return {label: dt.datetime.fromisoformat(ts) for label, ts in state.items()}
    except (json.JSONDecodeError, IOError):
        logging.warning("Could not read state file. Starting fresh.")
        return {}

def save_pipeline_state(state: dict):
    """Saves the latest timestamps to the state file."""
    try:
        with open(STATE_FILE_PATH, 'w') as f:
            serializable_state = {label: ts.isoformat() for label, ts in state.items()}
            json.dump(serializable_state, f, indent=4)
        logging.info(f"Successfully saved pipeline state.")
    except IOError:
        logging.error(f"Could not write to state file.")

# --- Core Orchestration Logic ---

def trigger_and_wait(client: ApifyClient, actor_id: str, run_input: dict, label: str, source: DataSource) -> dict | None:
    """Triggers an Apify actor and waits for it to finish."""
    try:
        logging.info(f"Triggering actor '{actor_id}' for label '{label}' with input: {run_input}")
        run = client.actor(actor_id).call(run_input=run_input)
        logging.info(f"Actor '{actor_id}' started! Run ID: {run['id']}")
        
        logging.info(f"Waiting for run {run['id']} to complete...")
        run_details = client.run(run['id']).wait_for_finish()

        if run_details['status'] == 'SUCCEEDED':
            logging.info(f"âœ… Apify run {run['id']} completed successfully.")
            return {'run_id': run['id'], 'source': source, 'label': label}
        else:
            logging.error(f"âŒ Apify run {run['id']} did not succeed. Status: {run_details['status']}.")
            return None
    except Exception:
        logging.error(f"An error occurred while running actor {actor_id}: {traceback.format_exc()}")
        return None

def main_cycle():
    """ Main function to run one full cycle of the data pipeline. """
    logging.info("--- Starting New Data Pipeline Cycle ---")

    try:
        logging.info("Performing pre-flight check to ensure database is initialized...")
        storage = SqliteMinerStorage(database=DATABASE_NAME)
        logging.info("âœ… Pre-flight check complete. Database is ready.")
    except Exception:
        logging.error(f"âŒ Pre-flight check failed. Could not initialize database. Aborting cycle.")
        logging.error(traceback.format_exc())
        return

    """ Main function to run one full cycle of the data pipeline. """
    if not APIFY_TOKEN:
        logging.error("APIFY_TOKEN is not set. Exiting.")
        sys.exit(1)

    logging.info("--- Starting New Data Pipeline Cycle ---")
    client = ApifyClient(APIFY_TOKEN)
    pipeline_state = load_pipeline_state()
    per_run_limits = calculate_per_run_limits()
    
    logging.info("Fetching dynamic targets...")
    try:
        targets = get_scraping_targets()
        scraper_configs = targets.get('scraper_configs', {})
    except Exception as e:
        logging.error(f"Could not get dynamic targets: {e}. Aborting cycle.")
        return

    successful_run_infos = []

    # Trigger X Scraper
    if 'X.flash' in scraper_configs:
        num_targets = len(scraper_configs['X.flash'].labels_to_scrape)
        limit_per_target = per_run_limits['x_per_run_limit'] // num_targets if num_targets > 0 else 0
        for config in scraper_configs['X.flash'].labels_to_scrape:
            if config.label_choices:
                label = random.choice(config.label_choices)
                # --- DEFINITIVE FIX: Add 'sort' parameter and ensure maxItems is at least 50 ---
                run_input = {
                    "searchTerms": [label], 
                    "maxItems": max(50, limit_per_target),
                    "sort": "Latest" 
                }
                # --- END FIX ---
                if label in pipeline_state: run_input["since"] = pipeline_state[label].strftime('%Y-%m-%d')
                run_info = trigger_and_wait(client, X_SCRAPER_ACTOR_ID, run_input, label, DataSource.X)
                if run_info: successful_run_infos.append(run_info)

    # Trigger Reddit Scraper
    if 'Reddit.lite' in scraper_configs:
        num_targets = len(scraper_configs['Reddit.lite'].labels_to_scrape)
        limit_per_target = per_run_limits['reddit_per_run_limit'] // num_targets if num_targets > 0 else 0
        for config in scraper_configs['Reddit.lite'].labels_to_scrape:
            if config.label_choices:
                label = random.choice(config.label_choices)
                run_input = {"searches": [label], "maxItems": limit_per_target, "sortBy": "new"}
                run_info = trigger_and_wait(client, REDDIT_SCRAPER_ACTOR_ID, run_input, label, DataSource.REDDIT)
                if run_info: successful_run_infos.append(run_info)

    # Trigger YouTube Discovery (Stage 1)
    youtube_video_urls = []
    if 'YouTube.custom.transcript' in scraper_configs:
        num_targets = len(scraper_configs['YouTube.custom.transcript'].labels_to_scrape)
        limit_per_target = per_run_limits['youtube_per_run_limit'] // num_targets if num_targets > 0 else 0
        for config in scraper_configs['YouTube.custom.transcript'].labels_to_scrape:
            if config.label_choices:
                label = random.choice(config.label_choices)
                run_input = {"searchQueries": [label], "maxVideos": limit_per_target}
                if label in pipeline_state: run_input["uploadedAfter"] = "7_days_ago"
                run_info = trigger_and_wait(client, YOUTUBE_DISCOVERY_ACTOR_ID, run_input, label, DataSource.YOUTUBE)
                if run_info:
                    dataset = client.run(run_info['run_id']).dataset().list_items().items
                    for item in dataset:
                        url = item.get('url') or item.get('videoUrl')
                        if url: youtube_video_urls.append(url)
                    client.dataset(client.run(run_info['run_id']).get()['defaultDatasetId']).delete()

    # Trigger YouTube Transcripts (Stage 2)
    if youtube_video_urls:
        run_input = {"video_urls": [{"url": url} for url in youtube_video_urls]}
        run_info = trigger_and_wait(client, YOUTUBE_TRANSCRIPT_ACTOR_ID, run_input, "transcript_batch", DataSource.YOUTUBE)
        if run_info:
            run_info['source'] = 'YOUTUBE_TRANSCRIPTS' 
            successful_run_infos.append(run_info)

    # ETL PHASE & UPDATE STATE
    if successful_run_infos:
        logging.info("Starting the ETL process...")
        etl_infos = [{'run_id': info['run_id'], 'source': (DataSource.YOUTUBE if info['source'] == 'YOUTUBE_TRANSCRIPTS' else info['source']), 'label': info['label']} for info in successful_run_infos]
        new_timestamps = run_etl(etl_infos)
        pipeline_state.update(new_timestamps)
        save_pipeline_state(pipeline_state)
        logging.info("ETL process finished.")
    else:
        logging.error("All scrapers failed. Skipping ETL.")

    logging.info("--- Data Pipeline Cycle Finished ---")

if __name__ == "__main__":
    main_cycle()
