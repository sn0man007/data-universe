import os
import json
import logging
import requests
# --- CORRECTED: Import LabelScrapingConfig ---
from scraping.config.model import ScraperConfig, LabelScrapingConfig

# Configure logging for this module
logger = logging.getLogger(__name__)

# --- Constants ---
# The path to the file generated by the miner's --gravity feature.
_DYNAMIC_TARGETS_FILE = os.path.expanduser('~/projects/data-universe/dynamic_desirability/total.json')
CACHE_FILE_PATH = os.path.join(os.path.dirname(__file__), "cached_targets.json")

# =============================================================================
# ==== Tier 3: Static Fallback Targets ====
# =============================================================================
def _get_static_fallback_targets():
    """
    Provides a complete, hardcoded list of high-value targets.
    This serves as the baseline for the hybrid intelligence model.
    """
    logger.info("Loading static fallback targets as a baseline.")
    return {
        "scraper_configs": {
            "X.flash": ScraperConfig(
                scraper_id="X.flash",
                cadence_seconds=300,
                # --- DEFINITIVE FIX: Use the correct 'label_choices' structure ---
                labels_to_scrape=[
                    LabelScrapingConfig(label_choices=['#bittensor', '#tao', '#crypto', '#btc', '#bitcoin'])
                ],
                url="https://x.com",
                metadata={"priority": "high", "source": "static_fallback"}
            ),
            "Reddit.lite": ScraperConfig(
                scraper_id="Reddit.lite",
                cadence_seconds=300,
                # --- DEFINITIVE FIX: Use the correct 'label_choices' structure ---
                labels_to_scrape=[
                    LabelScrapingConfig(label_choices=['Bitcoin', 'bittensor', 'cryptocurrency'])
                ],
                url="https://reddit.com",
                metadata={"priority": "high", "source": "static_fallback"}
            ),
            "YouTube.custom.transcript": ScraperConfig(
                scraper_id="YouTube.custom.transcript",
                cadence_seconds=300,
                # --- DEFINITIVE FIX: Use the correct 'label_choices' structure ---
                labels_to_scrape=[
                    LabelScrapingConfig(label_choices=['bittensor', 'decentralized ai', 'opentensor'])
                ],
                url="https://youtube.com",
                metadata={"priority": "high", "source": "static_fallback"}
            )
        }
    }

# =============================================================================
# ==== Tier 1: Dynamic Target Fetching (Hybrid Intelligence) ====
# =============================================================================
def _fetch_from_dynamic_source():
    """
    Implements a hybrid strategy:
    1. Reads the live desirability data from the 'total.json' file.
    2. If a platform has live data, it overwrites the static fallback for that platform.
    3. This ensures the miner is both reactive to the network and resilient.
    """
    logger.info(f"Attempting to read dynamic targets from '{_DYNAMIC_TARGETS_FILE}'...")

    # Start with our robust fallback list as a baseline.
    final_targets = _get_static_fallback_targets()
    final_configs = final_targets['scraper_configs']

    try:
        with open(_DYNAMIC_TARGETS_FILE, 'r') as f:
            desirability_data = json.load(f)
        logger.info(f"Successfully loaded {len(desirability_data)} live targets from the miner's file.")

        # --- Process the live data ---
        live_labels = {'x': set(), 'reddit': set(), 'youtube': set()}

        for item in desirability_data:
            params = item.get('params', {})
            label = params.get('label')
            platform = params.get('platform', '').lower()

            if label and platform in live_labels:
                live_labels[platform].add(label)

        # --- Hybrid Logic: Merge live data over the static baseline ---
        if live_labels['x']:
            logger.info("Overwriting static X targets with live network data.")
            final_configs['X.flash'].labels_to_scrape = [LabelScrapingConfig(label_choices=list(live_labels['x']))]

        if live_labels['reddit']:
            logger.info("Overwriting static Reddit targets with live network data.")
            reddit_labels = [label.split('r/')[-1] for label in live_labels['reddit']]
            final_configs['Reddit.lite'].labels_to_scrape = [LabelScrapingConfig(label_choices=reddit_labels)]
        
        if live_labels['youtube']:
            logger.info("Overwriting static YouTube targets with live network data.")
            final_configs['YouTube.custom.transcript'].labels_to_scrape = [LabelScrapingConfig(label_choices=list(live_labels['youtube']))]

        return final_targets

    except (FileNotFoundError, json.JSONDecodeError) as e:
        raise IOError(f"Could not read or parse '{os.path.basename(_DYNAMIC_TARGETS_FILE)}'. Reason: {e}")


# =============================================================================
# ==== Tier 2: Local Cache Operations ====
# =============================================================================
def _read_from_cache():
    """Reads the cached targets and wraps them in the expected structure."""
    if not os.path.exists(CACHE_FILE_PATH):
        logger.info("Local cache file does not exist. Skipping.")
        return None
    
    try:
        with open(CACHE_FILE_PATH, 'r') as f:
            cached_data = json.load(f)
        
        cached_configs = {
            scraper_id: ScraperConfig(**config_data)
            for scraper_id, config_data in cached_data.items()
        }
        
        logger.info(f"Successfully loaded {len(cached_configs)} targets from local cache.")
        return {"scraper_configs": cached_configs}
    except (json.JSONDecodeError, TypeError, ValueError) as e:
        logger.error(f"Could not read or parse local cache file. Reason: {e}")
        return None


def _write_to_cache(targets: dict):
    """Writes the given targets to the local cache file, unwrapping them first."""
    try:
        configs_to_write = targets.get("scraper_configs", {})
        if not configs_to_write:
            logger.warning("Attempted to write empty target set to cache. Skipping.")
            return

        targets_to_serialize = {
            scraper_id: config.dict() for scraper_id, config in configs_to_write.items()
        }
        
        logger.info(f"Attempting to write {len(targets_to_serialize)} targets to cache.")
        with open(CACHE_FILE_PATH, 'w') as f:
            json.dump(targets_to_serialize, f, indent=4)
        logger.info("Successfully updated local cache.")
    except Exception as e:
        logger.error(f"Failed to write to local cache. Reason: {e}")

# =============================================================================
# ==== Public API Function (Orchestrator) ====
# =============================================================================
def get_scraping_targets():
    """
    Retrieves the list of scraping targets using a three-tiered fallback strategy.
    """
    # TIER 1: Try Dynamic Source (with Hybrid Intelligence)
    try:
        targets = _fetch_from_dynamic_source()
        _write_to_cache(targets)
        return targets
    except Exception as e:
        logger.warning(f"Dynamic source fetch failed: {e}. Falling back to local cache.")

    # TIER 2: Try Local Cache
    targets = _read_from_cache()
    if targets:
        return targets

    # TIER 3: Use Static Fallback
    logger.critical("CRITICAL: Falling back to Tier 3: Static Fallback Targets. Both dynamic source and cache are unavailable.")
    return _get_static_fallback_targets()
